{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cda39d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63af4e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torchvision\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec48f79",
   "metadata": {},
   "source": [
    "# CUDA cores goes brrrrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60848a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e15db",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb3d1d",
   "metadata": {},
   "source": [
    "## Loading the videos and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c20e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = [\"A\", \"B\", \"C\", \"L\", \"R\", \"U\"]\n",
    "data = []\n",
    "removed_videos = {}\n",
    "\n",
    "frames_per_video = 10\n",
    "\n",
    "for letter in letters:\n",
    "    \n",
    "    removed_videos[letter] = []\n",
    "    anno_df = pd.read_csv(\"data/ASL_letter_\" + letter + \"/annotations.csv\")\n",
    "    nr = 0\n",
    "    while True:\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            video = torchvision.io.read_video(\"data/ASL_letter_\" + letter + \"/videos/video_\" + str(nr) + \".mp4\")\n",
    "            if video[0].shape[1] == 480 and video[0].shape[2] == 640:\n",
    "                for i in range(frames_per_video):\n",
    "                    frame = np.random.randint(0, video[0].shape[0])\n",
    "                    \n",
    "                    frame_df = anno_df.loc[np.logical_and(anno_df['video_idx'] == nr, anno_df['frame'] == frame)]\n",
    "                    x = torch.tensor(frame_df['x'].values)\n",
    "                    y = torch.tensor(frame_df['y'].values)\n",
    "\n",
    "                    annotations = np.array([])\n",
    "                    for i in range(x.shape[0]):\n",
    "                        annotations = np.append(annotations, [x[i], y[i]])\n",
    "                        \n",
    "                    image = torch.movedim(video[0][frame, :, :, :].float(), (0,1,2), (2,1,0))\n",
    "                    data.append((image, torch.tensor(annotations, dtype=torch.float32)))\n",
    "            else:\n",
    "                removed_videos[letter].append(nr)\n",
    "                \n",
    "            nr += 1\n",
    "            \n",
    "        except RuntimeError:\n",
    "            break\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b4ee5",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a094650",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "training_data = []\n",
    "validation_data = []\n",
    "\n",
    "total_videos = 0\n",
    "for letter in letters:\n",
    "    _, _, files = next(os.walk(\"data/ASL_letter_\" + letter + \"/videos\"))\n",
    "    file_count = len(files) - len(removed_videos[letter])\n",
    "    training_data = training_data + data[total_videos*frames_per_video:total_videos*frames_per_video + frames_per_video*int(split*file_count)]\n",
    "    validation_data = validation_data + data[total_videos*frames_per_video + frames_per_video*int(split*file_count):total_videos*frames_per_video + frames_per_video*file_count]\n",
    "    total_videos += file_count\n",
    "\n",
    "len(training_data), len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213be6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('training_data', 'wb')\n",
    "pickle.dump(training_data, file)\n",
    "file.close()\n",
    "\n",
    "file = open('validation_data', 'wb')\n",
    "pickle.dump(validation_data, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1d74317",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('training_data', 'rb')\n",
    "training_data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('validation_data', 'rb')\n",
    "validation_data = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8879b",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bee6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GesturesDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, d):\n",
    "        self.data = d\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        datapoint = self.data[idx]\n",
    "        return datapoint[0], datapoint[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93866e",
   "metadata": {},
   "source": [
    "## Initializing DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b2b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "training_set = GesturesDataset(training_data)\n",
    "validation_set = GesturesDataset(validation_data)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_set, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af858a27",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5f7c8",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d902b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(6, 1, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=18369, out_features=1000, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): Linear(in_features=1000, out_features=42, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):  \n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(18369, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 42)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)\n",
    "    \n",
    "model = SimpleCNN().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2,2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.group1layer1 = nn.Conv2d(3,3,3, padding=1, padding_mode='reflect')\n",
    "        #self.group1layer2 = nn.Conv2d(3,3,3, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.group2layer1 = nn.Conv2d(3,6,3, padding=1, padding_mode='reflect')\n",
    "        #self.group2layer2 = nn.Conv2d(6,6,3, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.group3layer1 = nn.Conv2d(6,12,3, padding=1, padding_mode='reflect')\n",
    "        #self.group3layer2 = nn.Conv2d(12,12,3, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.group4layer1 = nn.Conv2d(12,24,3, padding=1, padding_mode='reflect')\n",
    "        #self.group4layer2 = nn.Conv2d(24,24,3, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.group5layer1 = nn.Conv2d(24,48,3, padding=1, padding_mode='reflect')\n",
    "        #self.group5layer2 = nn.Conv2d(48,48,3, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.midlayer1 = nn.Conv2d(48,48,2, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.group6layer1 = nn.Conv2d(72,24,3, padding=1, padding_mode='reflect')\n",
    "        #self.group6layer2 = nn.Conv2d(24,24,3, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.midlayer2 = nn.Conv2d(24,24,2, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.group7layer1 = nn.Conv2d(36,12,3, padding=1, padding_mode='reflect')\n",
    "        #self.group7layer2 = nn.Conv2d(12,12,3, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.midlayer3 = nn.Conv2d(12,12,2, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.group8layer1 = nn.Conv2d(18,6,3, padding=1, padding_mode='reflect')\n",
    "        #self.group8layer2 = nn.Conv2d(6,6,3, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.midlayer4 = nn.Conv2d(6,6,2, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.group9layer1 = nn.Conv2d(9,1,3, padding=1, padding_mode='reflect')\n",
    "       # self.group9layer2 = nn.Conv2d(3,3,3, padding=1, padding_mode='reflect')\n",
    "        \n",
    "        self.ann_layer1 = nn.Linear(307200, 1000)\n",
    "        self.ann_layer2 = nn.Linear(1000, 500)\n",
    "        self.ann_layer3 = nn.Linear(500, 42)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.relu(self.group1layer1(x))\n",
    "        x2 = self.relu(self.group2layer1(self.maxpool(x1)))\n",
    "        x3 = self.relu(self.group3layer1(self.maxpool(x2)))\n",
    "        x4 = self.relu(self.group4layer1(self.maxpool(x3)))\n",
    "        x5 = self.relu(self.group5layer1(self.maxpool(x4)))\n",
    "        \n",
    "        y1 = self.relu(self.group6layer1(torch.cat((x4,self.midlayer1(self.upsample(x5))[:,:,:-1,:-1]), dim=1)))\n",
    "        y2 = self.relu(self.group7layer1(torch.cat((x3,self.midlayer2(self.upsample(y1))[:,:,:-1,:-1]), dim=1)))\n",
    "        y3 = self.relu(self.group8layer1(torch.cat((x2,self.midlayer3(self.upsample(y2))[:,:,:-1,:-1]), dim=1)))\n",
    "        y4 = self.relu(self.group9layer1(torch.cat((x1,self.midlayer4(self.upsample(y3))[:,:,:-1,:-1]), dim=1)))\n",
    "        \n",
    "        return self.ann_layer3(self.relu(self.ann_layer2(self.relu(self.ann_layer1(self.flatten(y4))))))\n",
    "\n",
    "model = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d84a459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming (1,3,640,480) tensor\n",
    "def L2Loss(Y, Y_pred):\n",
    "    return torch.sum(torch.pow(torch.sub(Y, Y_pred), 2))\n",
    "\n",
    "loss_fn = L2Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dca67b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd12c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "losses = np.array([])\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "model.train()  # set model in training mode\n",
    "\n",
    "# loop over the dataset multiple times, similar to our \"steps\" used before\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        \n",
    "        outputs = model(images.to(device))\n",
    "        loss = loss_fn(outputs, labels.to(device))\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        losses = np.append(losses, loss.item())\n",
    "        print(f'Epoch: {epoch+1} \\t Batch: {i+1} \\t Loss: {loss.item()}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d50058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"first_good_simplecnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac69fc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "model.load_state_dict(torch.load('first_good_simplecnn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650d528",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e9bf160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct percent:  0.5142857142857142\n"
     ]
    }
   ],
   "source": [
    "correct = np.array([])\n",
    "for i, (images, labels) in enumerate(validation_dataloader):\n",
    "    \n",
    "    outputs = model(images.to(device))\n",
    "    with torch.no_grad():\n",
    "        for j in range(outputs.shape[0]):\n",
    "            if loss_fn(outputs[j,:], labels[j,:].to(device)) < 15000:\n",
    "                correct = np.append(correct, 1)\n",
    "            else:\n",
    "                correct = np.append(correct, 0)\n",
    "\n",
    "print(\"Correct percent: \", np.mean(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4854a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, annotations = validation_set.__getitem__(np.random.randint(0,len(validation_set)))\n",
    "prediction = model(image[None,:,:,:].to(device))\n",
    "loss = loss_fn(prediction, annotations.to(device))\n",
    "print(loss.item())\n",
    "img = transform(image)\n",
    "drawing = ImageDraw.Draw(img)\n",
    "ellips_coords = []\n",
    "for i in range(0,42,2):\n",
    "    drawing.ellipse([(annotations[i] - 2, annotations[i+1] - 2), (annotations[i] + 2, annotations[i+1] + 2)], outline=(0,255,0))\n",
    "    drawing.ellipse([(prediction[0,i] - 2, prediction[0,i+1] - 2), (prediction[0,i] + 2, prediction[0,i+1] + 2)], outline=(0,0,255))\n",
    "\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbf2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
